{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network\n",
    "# Choose an appropriate activation function for each layer\n",
    "# Determine the number of nodes in each layer\n",
    "# Choose an appropriate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained neural network to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "# from d2l import torch as d2l\n",
    "from torch.utils.data import DataLoader,random_split,Dataset, SubsetRandomSampler, TensorDataset\n",
    "# from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=2, output_size=1):\n",
    "        super().__init__()\n",
    "        self.model=nn.Sequential(nn.Linear(input_size, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(32),\n",
    "                                nn.Linear(32, 64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(64),\n",
    "                                nn.Linear(64, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(128),\n",
    "                                nn.Linear(128, 256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(256),\n",
    "                                nn.Linear(256, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(128),\n",
    "                                nn.Linear(128, 64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(64),\n",
    "                                nn.Linear(64, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm1d(32),\n",
    "                                nn.Linear(32, output_size)\n",
    "                                )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Creating Optuna object and defining its parameters\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials = 30)\n",
    "# Showing optimization results\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial parameters:', study.best_trial.params)\n",
    "# print('Best score:', study.best_value)\n",
    "\n",
    "\n",
    "# Number of finished trials: 30\n",
    "# Best trial parameters: {'learning_rate': 0.0027315610999692746, 'optimizer': 'SGD', 'batch_size': 240, 'num_hidden_1': 408, 'num_hidden_2': 31, 'num_hidden_3': 886, 'num_hidden_4': 345, 'num_hidden_5': 464, 'num_hidden_6': 366}\n",
    "# Best score: 0.022719970179928675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size=2, output_size=1):\n",
    "#         super().__init__()\n",
    "#         layers = []\n",
    "#         neurons = [32, 64, 128, 64] * 3 + [32, 16, 8, 4, 2]\n",
    "#         for neuron in neurons:\n",
    "#             layers.append(nn.Linear(input_size, neuron))\n",
    "#             layers.append(nn.Tanh())\n",
    "#             layers.append(nn.BatchNorm1d(neuron))\n",
    "#             input_size = neuron\n",
    "#         layers.append(nn.Linear(input_size, output_size))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size=2, output_size=1):\n",
    "#         super().__init__()\n",
    "#         self.input_layer = nn.Linear(input_size, 32)\n",
    "#         self.hidden_layer1 = nn.Linear(32, 64)\n",
    "#         self.hidden_layer2 = nn.Linear(64, 128)\n",
    "#         self.hidden_layer3 = nn.Linear(128, 64)\n",
    "#         self.hidden_layer4 = nn.Linear(64, 32)\n",
    "#         self.output_layer = nn.Linear(32, output_size)\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.bn1 = nn.BatchNorm1d(32)\n",
    "#         self.bn2 = nn.BatchNorm1d(64)\n",
    "#         self.bn3 = nn.BatchNorm1d(128)\n",
    "#         self.bn4 = nn.BatchNorm1d(64)\n",
    "#         self.bn5 = nn.BatchNorm1d(32)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.bn1(self.tanh(self.input_layer(x)))\n",
    "#         x = self.bn2(self.tanh(self.hidden_layer1(x)))\n",
    "#         x = self.bn3(self.tanh(self.hidden_layer2(x)))\n",
    "#         x = self.bn4(self.tanh(self.hidden_layer3(x)))\n",
    "#         x = self.bn5(self.tanh(self.hidden_layer4(x)))\n",
    "#         x = self.output_layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device) -> float:\n",
    "    num_batches = len(dataloader) # batches per epoch\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output,y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, dataloader, loss_fn, device) -> float:\n",
    "    num_batches = len(dataloader) # batches per epoch\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.inference_mode(mode=True):\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            valid_loss += loss.item()\n",
    "    return valid_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler: torch.optim.lr_scheduler,\n",
    "def train(model, train_loader, valid_loader, loss_fn, optimizer, scheduler, epochs, device):\n",
    "    # Init the results\n",
    "    result = defaultdict(list)\n",
    "    # Set the model to the device\n",
    "    model.to(device)\n",
    "    # Iterate over the epochs\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        # Train the model\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        # Validate the model\n",
    "        valid_loss = valid_epoch(model, valid_loader, loss_fn, device)\n",
    "        # Record the loss\n",
    "        result[\"train_loss\"].append(train_loss)\n",
    "        result[\"valid_loss\"].append(valid_loss)\n",
    "        # Adjust the learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step(valid_loss)\n",
    "    # Return the results\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(result):\n",
    "    train_loss = result[\"train_loss\"]\n",
    "    valid_loss = result[\"valid_loss\"]\n",
    "    epochs = range(len(result[\"train_loss\"]))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, valid_loss, label=\"valid_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "x = torch.tensor(df.drop(['id','y'],axis=1).values,dtype=torch.float32).view(-1, 2)\n",
    "y = torch.tensor(df['y'].values,dtype=torch.float32).view(-1, 1)\n",
    "train_data = TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=K)\n",
    "model = Model()\n",
    "for fold_i, (train_idx, val_idx) in enumerate(kfold.split(train_data)):\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "        valid_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=valid_sampler)\n",
    "\n",
    "        # model = Model()\n",
    "        # loss_fn = nn.HuberLoss(reduction=\"mean\")\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.6)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100)\n",
    "        # scheduler = scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50)\n",
    "        # scheduler, \n",
    "        result = train(model, train_loader, valid_loader, loss_fn, optimizer, scheduler, EPOCHS, device)\n",
    "        print(f\"KFold: {fold_i+1} Train loss: {result['train_loss'][-1]} Valid loss: {result['valid_loss'][-1]}\")\n",
    "        models.append(model.state_dict())\n",
    "        plot(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100)\n",
    "losses = []\n",
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output = model(x)\n",
    "    loss = loss_fn(output,y)\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_x = torch.tensor(test_data.drop(['id'],axis=1).values,dtype=torch.float32).view(-1, 2).to(device)\n",
    "\n",
    "model = Model().to(device)\n",
    "outputs = np.zeros(len(test_x))\n",
    "\n",
    "for model_i in models:\n",
    "    # Load model state dict and set to eval mode\n",
    "    model.load_state_dict(model_i)\n",
    "    model.eval()\n",
    "    # Convert tensor to numpy array\n",
    "    outputs += model(test_x).cpu().detach().numpy().flatten()\n",
    "# Average predictions\n",
    "outputs /= len(models)\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\"id\": range(1, len(outputs) + 1), \"y\": outputs})\n",
    "# Save submission file\n",
    "submission.to_csv(\"./submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_path = 'submission.csv'\n",
    "y_best_path = 'submission_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.read_csv(y_pred_path)\n",
    "y_best = pd.read_csv(y_best_path)\n",
    "y_pred = y_pred.iloc[:, -1].values\n",
    "y_best = y_best.iloc[:, -1].values\n",
    "print(np.mean((y_pred - y_best) ** 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
